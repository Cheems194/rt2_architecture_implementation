{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 23370,
     "status": "ok",
     "timestamp": 1765683482143,
     "user": {
      "displayName": "Harjeet Cheema",
      "userId": "14168617003254068455"
     },
     "user_tz": -330
    },
    "id": "WMVGgTTMHkBc"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, Sampler\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import PreTrainedTokenizer\n",
    "from typing import List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel, GPT2Config, GPT2LMHeadModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 471,
     "status": "ok",
     "timestamp": 1765683482616,
     "user": {
      "displayName": "Harjeet Cheema",
      "userId": "14168617003254068455"
     },
     "user_tz": -330
    },
    "id": "gfUH5-62DaqN",
    "outputId": "3c27d981-33a9-458b-e42c-23fdce3b7b67"
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 6\n",
    "LR = 3e-4\n",
    "WARMUP_STEPS = 500\n",
    "MAX_STEPS = 30000\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\n",
    "    \"/content/drive/MyDrive/Colab Notebooks/RT-2/rt2_tokenizer\"\n",
    ")\n",
    "\n",
    "print(\"Tokenizer size:\", len(tokenizer))\n",
    "print(\"EOS ID:\", tokenizer.eos_token_id)\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1765683482621,
     "user": {
      "displayName": "Harjeet Cheema",
      "userId": "14168617003254068455"
     },
     "user_tz": -330
    },
    "id": "_SJCGzLxW2Y0"
   },
   "outputs": [],
   "source": [
    "class RobotDataset(Dataset):\n",
    "    def __init__(self, jsonl_path: str, tokenizer: PreTrainedTokenizer, image_size=224):\n",
    "        self.samples = []\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                self.samples.append(json.loads(line))\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.act_start_id = tokenizer.convert_tokens_to_ids(\"<act_start>\")\n",
    "        self.eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        image = Image.open(sample[\"image\"]).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        instr_ids = self.tokenizer.encode(\n",
    "            sample[\"instruction\"],\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        input_ids = (\n",
    "            [self.tokenizer.bos_token_id]\n",
    "            + instr_ids\n",
    "            + [self.act_start_id]\n",
    "        )\n",
    "\n",
    "        target_ids = sample[\"action_tokens\"] + [self.eos_id]\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(target_ids, dtype=torch.long),\n",
    "            \"type\": \"robot\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1765683482639,
     "user": {
      "displayName": "Harjeet Cheema",
      "userId": "14168617003254068455"
     },
     "user_tz": -330
    },
    "id": "s5G97P-ducYP"
   },
   "outputs": [],
   "source": [
    "class VLMDataset(Dataset):\n",
    "    def __init__(self, jsonl_path: str, tokenizer: PreTrainedTokenizer, image_size=224):\n",
    "        self.samples = []\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                self.samples.append(json.loads(line))\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        image = Image.open(sample[\"image\"]).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        input_ids = [self.tokenizer.bos_token_id]\n",
    "        target_ids = self.tokenizer.encode(\n",
    "            sample[\"caption\"],\n",
    "            add_special_tokens=False\n",
    "        ) + [self.tokenizer.eos_token_id]\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(target_ids, dtype=torch.long),\n",
    "            \"type\": \"vlm\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1765683482943,
     "user": {
      "displayName": "Harjeet Cheema",
      "userId": "14168617003254068455"
     },
     "user_tz": -330
    },
    "id": "imBn5temb8xF"
   },
   "outputs": [],
   "source": [
    "def get_robot_ratio(epoch):\n",
    "    if epoch < 2:\n",
    "        return 0.6\n",
    "    elif epoch < 4:\n",
    "        return 0.7\n",
    "    else:\n",
    "        return 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1765683482945,
     "user": {
      "displayName": "Harjeet Cheema",
      "userId": "14168617003254068455"
     },
     "user_tz": -330
    },
    "id": "9v1Ej6syugUP"
   },
   "outputs": [],
   "source": [
    "class MixedBatchSampler(Sampler[List[int]]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        robot_len: int,\n",
    "        vlm_len: int,\n",
    "        batch_size: int,\n",
    "        robot_ratio: float = 0.7\n",
    "    ):\n",
    "        self.robot_len = robot_len\n",
    "        self.vlm_len = vlm_len\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.robot_bs = int(batch_size * robot_ratio)\n",
    "        self.vlm_bs = batch_size - self.robot_bs\n",
    "\n",
    "    def __iter__(self):\n",
    "        robot_indices = list(range(self.robot_len))\n",
    "        vlm_indices = list(range(self.vlm_len))\n",
    "\n",
    "        random.shuffle(robot_indices)\n",
    "        random.shuffle(vlm_indices)\n",
    "\n",
    "        r_ptr, v_ptr = 0, 0\n",
    "\n",
    "        while r_ptr < self.robot_len and v_ptr < self.vlm_len:\n",
    "            batch = []\n",
    "\n",
    "            batch.extend(robot_indices[r_ptr:r_ptr+self.robot_bs])\n",
    "            batch.extend(\n",
    "                [i + self.robot_len for i in vlm_indices[v_ptr:v_ptr+self.vlm_bs]]\n",
    "            )\n",
    "\n",
    "            random.shuffle(batch)\n",
    "\n",
    "            yield batch\n",
    "\n",
    "            r_ptr += self.robot_bs\n",
    "            v_ptr += self.vlm_bs\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(\n",
    "            self.robot_len // self.robot_bs,\n",
    "            self.vlm_len // self.vlm_bs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1765683482969,
     "user": {
      "displayName": "Harjeet Cheema",
      "userId": "14168617003254068455"
     },
     "user_tz": -330
    },
    "id": "0IfIo9SNujJw"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_token_id, eos_token_id):\n",
    "    images = torch.stack([x[\"image\"] for x in batch])\n",
    "\n",
    "    input_ids = [x[\"input_ids\"] for x in batch]\n",
    "    labels = [x[\"labels\"] for x in batch]\n",
    "\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids,\n",
    "        batch_first=True,\n",
    "        padding_value=eos_token_id\n",
    "    )\n",
    "\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        labels,\n",
    "        batch_first=True,\n",
    "        padding_value=-100\n",
    "    )\n",
    "\n",
    "    attention_mask = (input_ids != eos_token_id).long()\n",
    "\n",
    "    return {\n",
    "        \"images\": images,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1765683741273,
     "user": {
      "displayName": "Harjeet Cheema",
      "userId": "14168617003254068455"
     },
     "user_tz": -330
    },
    "id": "oikLXDWiulun"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel, GPT2Config, GPT2LMHeadModel\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "class MiniRT2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        vision_model_name: str = \"google/vit-base-patch16-224\",\n",
    "        d_model: int = 768,\n",
    "        n_layer: int = 8,\n",
    "        n_head: int = 8,\n",
    "        max_seq_len: int = 128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vision_encoder = ViTModel.from_pretrained(vision_model_name)\n",
    "        self.vision_encoder.eval()\n",
    "        for p in self.vision_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        vision_dim = self.vision_encoder.config.hidden_size\n",
    "        self.vision_proj = nn.Linear(vision_dim, d_model)\n",
    "\n",
    "        config = GPT2Config(\n",
    "            vocab_size=vocab_size,\n",
    "            n_embd=d_model,\n",
    "            n_layer=n_layer,\n",
    "            n_head=n_head,\n",
    "            n_positions=max_seq_len + 256,\n",
    "            bos_token_id=None,\n",
    "            eos_token_id=None,\n",
    "        )\n",
    "\n",
    "        self.decoder = GPT2LMHeadModel(config)\n",
    "\n",
    "        self.loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        images,\n",
    "        input_ids,\n",
    "        attention_mask=None,\n",
    "        labels=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        images: (B, 3, 224, 224)\n",
    "        input_ids: (B, T)\n",
    "        attention_mask: (B, T)\n",
    "        labels: (B, T)\n",
    "        \"\"\"\n",
    "\n",
    "        B = images.size(0)\n",
    "\n",
    "        with autocast(enabled=False):\n",
    "            images = images.float()\n",
    "            vision_outputs = self.vision_encoder(pixel_values=images)\n",
    "            vision_embeds = self.vision_proj(\n",
    "                vision_outputs.last_hidden_state\n",
    "            )\n",
    "\n",
    "        token_embeds = self.decoder.transformer.wte(input_ids)\n",
    "\n",
    "        inputs_embeds = torch.cat(\n",
    "            [vision_embeds, token_embeds], dim=1\n",
    "        )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            vision_mask = torch.ones(\n",
    "                (B, vision_embeds.size(1)),\n",
    "                device=attention_mask.device,\n",
    "                dtype=attention_mask.dtype\n",
    "            )\n",
    "            attention_mask = torch.cat(\n",
    "                [vision_mask, attention_mask], dim=1\n",
    "            )\n",
    "\n",
    "        if labels is not None:\n",
    "            vision_label_pad = torch.full(\n",
    "                (B, vision_embeds.size(1)),\n",
    "                -100,\n",
    "                device=labels.device,\n",
    "                dtype=labels.dtype\n",
    "            )\n",
    "            labels = torch.cat(\n",
    "                [vision_label_pad, labels], dim=1\n",
    "            )\n",
    "\n",
    "        outputs = self.decoder(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            use_cache=False,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        if labels is not None:\n",
    "            shift_logits = outputs.logits[:, :-1, :]\n",
    "            shift_labels = labels[:, 1:]\n",
    "\n",
    "            min_len = min(\n",
    "                shift_logits.size(1),\n",
    "                shift_labels.size(1)\n",
    "            )\n",
    "\n",
    "            shift_logits = shift_logits[:, :min_len, :].contiguous()\n",
    "            shift_labels = shift_labels[:, :min_len].contiguous()\n",
    "\n",
    "            loss = self.loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "\n",
    "            outputs.loss = loss\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 173,
     "status": "ok",
     "timestamp": 1765683742381,
     "user": {
      "displayName": "Harjeet Cheema",
      "userId": "14168617003254068455"
     },
     "user_tz": -330
    },
    "id": "ybk9LxnHCtrl"
   },
   "outputs": [],
   "source": [
    "robot_ds = RobotDataset(\n",
    "    jsonl_path=\"/content/drive/MyDrive/Colab Notebooks/RT-2/output/train_sequences_fixed.jsonl\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "vlm_ds = VLMDataset(\n",
    "    jsonl_path=\"/content/drive/MyDrive/Colab Notebooks/RT-2/vlm_dataset.jsonl\",\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1765683743275,
     "user": {
      "displayName": "Harjeet Cheema",
      "userId": "14168617003254068455"
     },
     "user_tz": -330
    },
    "id": "a7dvKx6NFYnx"
   },
   "outputs": [],
   "source": [
    "class CombinedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, robot_ds, vlm_ds):\n",
    "        self.robot_ds = robot_ds\n",
    "        self.vlm_ds = vlm_ds\n",
    "        self.robot_len = len(robot_ds)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.robot_len + len(self.vlm_ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < self.robot_len:\n",
    "            return self.robot_ds[idx]\n",
    "        else:\n",
    "            return self.vlm_ds[idx - self.robot_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1765683744580,
     "user": {
      "displayName": "Harjeet Cheema",
      "userId": "14168617003254068455"
     },
     "user_tz": -330
    },
    "id": "LRBo-znuc5_d"
   },
   "outputs": [],
   "source": [
    "combined_ds = CombinedDataset(robot_ds, vlm_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3018,
     "status": "ok",
     "timestamp": 1765683755940,
     "user": {
      "displayName": "Harjeet Cheema",
      "userId": "14168617003254068455"
     },
     "user_tz": -330
    },
    "id": "K1TlTYTlc_a5",
    "outputId": "47d50519-5373-4c3c-c6d9-c57041aa30e3"
   },
   "outputs": [],
   "source": [
    "model = MiniRT2(\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_layer=8,\n",
    "    n_head=8,\n",
    "    d_model=768\n",
    ")\n",
    "\n",
    "# Ensure vocab alignment\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model.vision_encoder = model.vision_encoder.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1765683755943,
     "user": {
      "displayName": "Harjeet Cheema",
      "userId": "14168617003254068455"
     },
     "user_tz": -330
    },
    "id": "4Pdh7gv8dEHP"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LR,\n",
    "    weight_decay=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1765683756146,
     "user": {
      "displayName": "Harjeet Cheema",
      "userId": "14168617003254068455"
     },
     "user_tz": -330
    },
    "id": "MQ6Y29vydMgJ"
   },
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=MAX_STEPS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1765683756771,
     "user": {
      "displayName": "Harjeet Cheema",
      "userId": "14168617003254068455"
     },
     "user_tz": -330
    },
    "id": "g3cOa4rMdQgv",
    "outputId": "fe6c015a-e896-4c06-ee0b-f2e52fcbefc8"
   },
   "outputs": [],
   "source": [
    "scaler = GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aED9JpiMdTRj",
    "outputId": "4daa37a7-f557-495e-ca2a-09635bdd2598"
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    robot_ratio = get_robot_ratio(epoch)\n",
    "    print(f\"Epoch {epoch+1}: robot_ratio={robot_ratio}\")\n",
    "\n",
    "    sampler = MixedBatchSampler(\n",
    "        robot_len=len(robot_ds),\n",
    "        vlm_len=len(vlm_ds),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        robot_ratio=robot_ratio\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(\n",
    "    combined_ds,\n",
    "    batch_sampler=sampler,\n",
    "    collate_fn=lambda b: collate_fn(\n",
    "        b,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    ),\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for batch in pbar:\n",
    "        if global_step >= MAX_STEPS:\n",
    "            break\n",
    "\n",
    "        images = batch[\"images\"].to(DEVICE)\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(\n",
    "                images=images,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{loss.item():.4f}\",\n",
    "            \"step\": global_step\n",
    "        })\n",
    "\n",
    "    torch.save({\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"step\": global_step\n",
    "    }, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "\n",
    "    if global_step >= MAX_STEPS:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSEPgoGDye8N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNjhiP21U+j5oxF744c7N0v",
   "gpuType": "T4",
   "mount_file_id": "1vSo1aeTBMR80c2WjQ67AM5EHVqDQmJwu",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
